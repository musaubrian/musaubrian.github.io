<!DOCTYPE html>
<html lang="en"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width"><meta name="description" content="LLMs on your machine" />

<title>
    
    LLMs locally | CoreDump
    
</title>












<link rel="stylesheet" href="/assets/combined.min.06be76fa25e4ecb4d473373b74760273efe3314b6eb608effa9c2b6a617d92e2.css" media="all">



  </head>

  

  
  
  

  <body class="auto">

    <div class="content">
      <header>
        

<div class="header">

    

</div>
      </header>

      <main class="main">
        




<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a href="/posts/">Posts</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a class="breadcrumbs-current" href="/posts/host-your-own-llm/">LLMs locally</a>
</div>


<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">LLMs locally</h1>
    
    <p class="single-summary">Install and run large language models locally using Ollama.</p>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2024-07-08T00:00:00&#43;00:00">July 8, 2024</time>
      

      
      &nbsp; · &nbsp;
      1 min read
      
    </p>

  </div>

  

  
  

  <div class="single-tags">
    
    <span>
      <a href="//localhost:1313/tags/llms/">#LLMs</a>
    </span>
    
    
  </div>

  
  

  

  

  

  <div class="single-content">
    <h2 id="intro">Intro</h2>
<p>If you don&rsquo;t already know what <a href="https://ollama.com">ollama</a> is, I don&rsquo;t know which rock you&rsquo;ve been living under.</p>
<p>Basically its a simplified way of getting up and running with large language modals(<strong>LLMs</strong>) locally,
giving you ChatGPT like capabilities right on your very own device.</p>
<p>We are going to give you this same capabilities but on your machine.
Before we dive in, just a heads up:</p>
<blockquote>
<p>Ollama says you should have at least 8 GB of RAM to run the 3B models, 16 GB for the 7B models, and 32 GB for the 13B big boys.</p>
</blockquote>
<p>Just know that bigger numbers need beefier computers.</p>
<p>Follow the instructions on <a href="https://ollama.com/download">ollama</a> to install it, once installed go ahead and search for a model that fits your needs.</p>
<p>I am going to use the <code>neural-chat model</code>, its lightweight and can run in machines with less than 8gigs.</p>
<p>Download it using <code>ollama pull &lt;neural-chat&gt;</code> <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p><code>ollama pull</code> downloads the model from ollama&rsquo;s registry to your machine and stores in the following respective locations</p>
<ul>
<li>Linux: <code>/usr/share/ollama/.ollama/models</code></li>
<li>Windows: <code>C:\Users\%username%\.ollama\models</code></li>
<li>macOS: <code>~/.ollama/models</code></li>
</ul>
<p>Run the downloaded model using <code>ollama run &lt;neural-chat&gt;</code>.</p>
<p>That&rsquo;s it, you have your own instance of an llm on your machine.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>you can replace the contents of <code>&lt;neural-chat&gt;</code> with your prefered model&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    
  </div>

  

  
  

<div class="single-pagination">
    <hr />

    <div class="flex">

        <div class="single-pagination-prev">
            
            <div class="single-pagination-container-prev">
                <div class="single-pagination-text">←</div>
                <div class="single-pagination-text">
                    <a href="/posts/jade/">
                        jade.nvim
                    </a>
                </div>
            </div>
            
        </div>

        <div class="single-pagination-next">
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      <p>Powered by
    <a href="https://gohugo.io/">Hugo</a>
    and
    <a href="https://github.com/tomfran/typo">tomfran/typo</a>
</p>


    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>